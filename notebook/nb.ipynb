{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c142d81d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4e91ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict\n",
    "from fastprogress import progress_bar\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ExifTags\n",
    "import cv2\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "import torch\n",
    "import kornia\n",
    "from kornia.feature import (\n",
    "    LoFTR, LocalFeature, PassLAF, LAFOrienter, PatchDominantGradientOrientation, OriNet, LAFAffNetShapeEstimator,\n",
    "    KeyNetDetector, LAFDescriptor, HardNet8, HyNet, TFeat, SOSNet, get_laf_center, DescriptorMatcher\n",
    ")\n",
    "from transformers import TFViTModel, ViTFeatureExtractor\n",
    "\n",
    "import pycolmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1a4c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/home/gunes/Desktop/Kaggle/image-matching-challenge-2023')\n",
    "competition_dataset = root / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74b0177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(root / 'venv' / 'lib' / 'python3.9' / 'site-packages' / 'SuperGluePretrainedNetwork'))\n",
    "from models.matching import Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddf14666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (327, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(competition_dataset / 'sample_submission.csv')\n",
    "\n",
    "if df.shape[0] != 3:\n",
    "    # Enable submission mode and disable verbose\n",
    "    submission = True\n",
    "    verbose = False\n",
    "    train_or_test_directory = competition_dataset / 'test'\n",
    "else:\n",
    "    # Disable submission mode and enable verbose\n",
    "    submission = False\n",
    "    verbose = True\n",
    "    train_or_test_directory = competition_dataset / 'train'\n",
    "\n",
    "if submission is False:\n",
    "    df = pd.read_csv(competition_dataset / 'train' / 'train_labels.csv')\n",
    "    \n",
    "print(f'Dataset Shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4a3bc53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_angle_detection_model():\n",
    "    \n",
    "    \"\"\"\n",
    "    Load angle detection model and processor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras.engine.functional.Functional\n",
    "        Angle detection model\n",
    "        \n",
    "    processor: transformers.ViTFeatureExtractor\n",
    "        Image processor\n",
    "    \"\"\"\n",
    "    \n",
    "    vit = TFViTModel.from_pretrained(root / 'models' / 'vit-base-patch16-224')\n",
    "    \n",
    "    input_layer = Input(shape=(3, 224, 224))\n",
    "    x = vit(input_layer)\n",
    "    y = Dense(1, activation='linear')(x[-1])\n",
    "    \n",
    "    model = Model(input_layer, y)\n",
    "    model.load_weights(root / 'models' / 'deep-image-orientation-angle-detection' / 'model-vit-ang-loss.h5')\n",
    "    \n",
    "    processor = ViTFeatureExtractor.from_pretrained(root / 'models' / 'vit-base-patch16-224')\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def detect_angle(image, model, processor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Detect angle of a given image\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image: numpy.ndarray of shape (height, width, channel)\n",
    "        Image array\n",
    "        \n",
    "    model: keras.engine.functional.Functional\n",
    "        Angle detection model\n",
    "        \n",
    "    processor: transformers.ViTFeatureExtractor\n",
    "        Image processor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    outputs: float\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = processor(images=[image], return_tensors='np')['pixel_values']\n",
    "    outputs = model.predict(inputs, verbose=None)[0][0]\n",
    "        \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fde59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at /home/gunes/Desktop/Kaggle/image-matching-challenge-2023/models/vit-base-patch16-224 were not used when initializing TFViTModel: ['classifier']\n",
      "- This IS expected if you are initializing TFViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTModel were not initialized from the model checkpoint at /home/gunes/Desktop/Kaggle/image-matching-challenge-2023/models/vit-base-patch16-224 and are newly initialized: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████| 327/327 [00:40<00:00,  8.02it/s]\n"
     ]
    }
   ],
   "source": [
    "angle_detection_model, angle_detection_processor = load_angle_detection_model()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "    image_path = train_or_test_directory / row['image_path']\n",
    "    image = np.array(Image.open(image_path))\n",
    "    \n",
    "    # Extract image dimensions and memory usage\n",
    "    df.loc[idx, 'image_height'] = image.shape[0]\n",
    "    df.loc[idx, 'image_width'] = image.shape[1]\n",
    "    df.loc[idx, 'memory_usage'] = image.nbytes\n",
    "    \n",
    "    # Extract image orientation\n",
    "    angle = detect_angle(\n",
    "        image=image,\n",
    "        model=angle_detection_model,\n",
    "        processor=angle_detection_processor\n",
    "    )\n",
    "    df.loc[idx, 'angle'] = -angle\n",
    "    \n",
    "\n",
    "df['image_id'] = df['image_path'].apply(lambda x: str(x).split('/')[-1])\n",
    "df['memory_usage'] /= (1024 ** 2)\n",
    "\n",
    "del angle_detection_model, angle_detection_processor\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0965b0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>scene</th>\n",
       "      <th>image_path</th>\n",
       "      <th>rotation_matrix</th>\n",
       "      <th>translation_vector</th>\n",
       "      <th>image_height</th>\n",
       "      <th>image_width</th>\n",
       "      <th>memory_usage</th>\n",
       "      <th>image_id</th>\n",
       "      <th>angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urban</td>\n",
       "      <td>kyiv-puppet-theater</td>\n",
       "      <td>urban/kyiv-puppet-theater/images/IMG_20220127_...</td>\n",
       "      <td>0.961211667939937;0.012901284585988954;0.27550...</td>\n",
       "      <td>-15.272578207792487;-0.6299638684926413;21.960...</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>5.493164</td>\n",
       "      <td>IMG_20220127_170709.jpg</td>\n",
       "      <td>-0.264195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urban</td>\n",
       "      <td>kyiv-puppet-theater</td>\n",
       "      <td>urban/kyiv-puppet-theater/images/IMG_20220127_...</td>\n",
       "      <td>0.9983992256415107;0.03067020011152836;0.04752...</td>\n",
       "      <td>-10.86744827847772;5.142649233329267;22.080480...</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>5.493164</td>\n",
       "      <td>IMG_20220127_170633.jpg</td>\n",
       "      <td>-1.230195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urban</td>\n",
       "      <td>kyiv-puppet-theater</td>\n",
       "      <td>urban/kyiv-puppet-theater/images/IMG_20220127_...</td>\n",
       "      <td>-0.00887553412819142;-0.176226804404349;-0.984...</td>\n",
       "      <td>21.65171058540772;5.995022279588175;8.02005533...</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4.119873</td>\n",
       "      <td>IMG_20220127_170350.jpg</td>\n",
       "      <td>5.902641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urban</td>\n",
       "      <td>kyiv-puppet-theater</td>\n",
       "      <td>urban/kyiv-puppet-theater/images/IMG_20220127_...</td>\n",
       "      <td>-0.9161929266043187;-0.08177524286162957;-0.39...</td>\n",
       "      <td>9.821364270813396;4.839277289238847;19.6308283...</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4.119873</td>\n",
       "      <td>IMG_20220127_170339.jpg</td>\n",
       "      <td>2.854750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urban</td>\n",
       "      <td>kyiv-puppet-theater</td>\n",
       "      <td>urban/kyiv-puppet-theater/images/IMG_20220127_...</td>\n",
       "      <td>0.9917061795507187;0.02380210836276639;0.12630...</td>\n",
       "      <td>-25.98610607878878;1.9810845701842204;-1.04975...</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4.119873</td>\n",
       "      <td>IMG_20220127_170250.jpg</td>\n",
       "      <td>-21.354853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>heritage</td>\n",
       "      <td>wall</td>\n",
       "      <td>heritage/wall/images/DSC_5051_acr.jpg</td>\n",
       "      <td>0.9996327491725756;-0.023470477496970843;0.013...</td>\n",
       "      <td>-24.878330444800003;-0.307958687968;0.10009750...</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>69.767578</td>\n",
       "      <td>DSC_5051_acr.jpg</td>\n",
       "      <td>-0.084656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>heritage</td>\n",
       "      <td>wall</td>\n",
       "      <td>heritage/wall/images/DSC_5054_acr.jpg</td>\n",
       "      <td>0.998539039945242;-0.0008816636815971297;0.054...</td>\n",
       "      <td>-26.397828666400002;0.25433805800000003;1.0851...</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>69.767578</td>\n",
       "      <td>DSC_5054_acr.jpg</td>\n",
       "      <td>-0.805049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>heritage</td>\n",
       "      <td>wall</td>\n",
       "      <td>heritage/wall/images/DSC_5057_acr.jpg</td>\n",
       "      <td>0.9981761687264716;-0.008441133163217011;0.059...</td>\n",
       "      <td>-27.9128179912;0.08558456216799999;1.30660573568</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>69.767578</td>\n",
       "      <td>DSC_5057_acr.jpg</td>\n",
       "      <td>-1.323586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>heritage</td>\n",
       "      <td>wall</td>\n",
       "      <td>heritage/wall/images/DSC_5060_acr.jpg</td>\n",
       "      <td>0.9999248231217353;-0.010114127992376822;-0.00...</td>\n",
       "      <td>-29.1611086616;0.126412881824;-0.4765696612</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>69.767578</td>\n",
       "      <td>DSC_5060_acr.jpg</td>\n",
       "      <td>-0.437576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>heritage</td>\n",
       "      <td>wall</td>\n",
       "      <td>heritage/wall/images/DSC_5063_acr.jpg</td>\n",
       "      <td>0.9998168742332154;-0.009208346530131174;0.016...</td>\n",
       "      <td>-30.4180543408;0.204327655056;-0.0251494861696</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>6048.0</td>\n",
       "      <td>69.767578</td>\n",
       "      <td>DSC_5063_acr.jpg</td>\n",
       "      <td>0.019002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset                scene   \n",
       "0       urban  kyiv-puppet-theater  \\\n",
       "1       urban  kyiv-puppet-theater   \n",
       "2       urban  kyiv-puppet-theater   \n",
       "3       urban  kyiv-puppet-theater   \n",
       "4       urban  kyiv-puppet-theater   \n",
       "..        ...                  ...   \n",
       "322  heritage                 wall   \n",
       "323  heritage                 wall   \n",
       "324  heritage                 wall   \n",
       "325  heritage                 wall   \n",
       "326  heritage                 wall   \n",
       "\n",
       "                                            image_path   \n",
       "0    urban/kyiv-puppet-theater/images/IMG_20220127_...  \\\n",
       "1    urban/kyiv-puppet-theater/images/IMG_20220127_...   \n",
       "2    urban/kyiv-puppet-theater/images/IMG_20220127_...   \n",
       "3    urban/kyiv-puppet-theater/images/IMG_20220127_...   \n",
       "4    urban/kyiv-puppet-theater/images/IMG_20220127_...   \n",
       "..                                                 ...   \n",
       "322              heritage/wall/images/DSC_5051_acr.jpg   \n",
       "323              heritage/wall/images/DSC_5054_acr.jpg   \n",
       "324              heritage/wall/images/DSC_5057_acr.jpg   \n",
       "325              heritage/wall/images/DSC_5060_acr.jpg   \n",
       "326              heritage/wall/images/DSC_5063_acr.jpg   \n",
       "\n",
       "                                       rotation_matrix   \n",
       "0    0.961211667939937;0.012901284585988954;0.27550...  \\\n",
       "1    0.9983992256415107;0.03067020011152836;0.04752...   \n",
       "2    -0.00887553412819142;-0.176226804404349;-0.984...   \n",
       "3    -0.9161929266043187;-0.08177524286162957;-0.39...   \n",
       "4    0.9917061795507187;0.02380210836276639;0.12630...   \n",
       "..                                                 ...   \n",
       "322  0.9996327491725756;-0.023470477496970843;0.013...   \n",
       "323  0.998539039945242;-0.0008816636815971297;0.054...   \n",
       "324  0.9981761687264716;-0.008441133163217011;0.059...   \n",
       "325  0.9999248231217353;-0.010114127992376822;-0.00...   \n",
       "326  0.9998168742332154;-0.009208346530131174;0.016...   \n",
       "\n",
       "                                    translation_vector  image_height   \n",
       "0    -15.272578207792487;-0.6299638684926413;21.960...        1600.0  \\\n",
       "1    -10.86744827847772;5.142649233329267;22.080480...        1600.0   \n",
       "2    21.65171058540772;5.995022279588175;8.02005533...        1200.0   \n",
       "3    9.821364270813396;4.839277289238847;19.6308283...        1200.0   \n",
       "4    -25.98610607878878;1.9810845701842204;-1.04975...        1200.0   \n",
       "..                                                 ...           ...   \n",
       "322  -24.878330444800003;-0.307958687968;0.10009750...        4032.0   \n",
       "323  -26.397828666400002;0.25433805800000003;1.0851...        4032.0   \n",
       "324   -27.9128179912;0.08558456216799999;1.30660573568        4032.0   \n",
       "325        -29.1611086616;0.126412881824;-0.4765696612        4032.0   \n",
       "326     -30.4180543408;0.204327655056;-0.0251494861696        4032.0   \n",
       "\n",
       "     image_width  memory_usage                 image_id      angle  \n",
       "0         1200.0      5.493164  IMG_20220127_170709.jpg  -0.264195  \n",
       "1         1200.0      5.493164  IMG_20220127_170633.jpg  -1.230195  \n",
       "2         1200.0      4.119873  IMG_20220127_170350.jpg   5.902641  \n",
       "3         1200.0      4.119873  IMG_20220127_170339.jpg   2.854750  \n",
       "4         1200.0      4.119873  IMG_20220127_170250.jpg -21.354853  \n",
       "..           ...           ...                      ...        ...  \n",
       "322       6048.0     69.767578         DSC_5051_acr.jpg  -0.084656  \n",
       "323       6048.0     69.767578         DSC_5054_acr.jpg  -0.805049  \n",
       "324       6048.0     69.767578         DSC_5057_acr.jpg  -1.323586  \n",
       "325       6048.0     69.767578         DSC_5060_acr.jpg  -0.437576  \n",
       "326       6048.0     69.767578         DSC_5063_acr.jpg   0.019002  \n",
       "\n",
       "[327 rows x 10 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3aa417",
   "metadata": {},
   "source": [
    "## 2. Image Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f53e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=32)\n",
    "def read_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def create_image_pairs(image_paths):\n",
    "\n",
    "    \"\"\"\n",
    "    Create all possible image pairs from given list of image paths\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_paths: list of shape (n_images)\n",
    "        List of image paths\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image_pair_indices: list of shape (n_image_pairs)\n",
    "        List of tuples of image pair indices\n",
    "    \"\"\"\n",
    "\n",
    "    image_pair_indices = []\n",
    "\n",
    "    for i in range(len(image_paths)):\n",
    "        for j in range(i + 1, len(image_paths)):\n",
    "            image_pair_indices.append((i, j))\n",
    "\n",
    "    return image_pair_indices\n",
    "\n",
    "\n",
    "def resize_with_aspect_ratio(image, longest_edge):\n",
    "\n",
    "    \"\"\"\n",
    "    Resize image while preserving its aspect ratio\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: numpy.ndarray of shape (height, width, 3)\n",
    "        Image array\n",
    "\n",
    "    longest_edge: int\n",
    "        Desired number of pixels on the longest edge\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image: numpy.ndarray of shape (resized_height, resized_width, 3)\n",
    "        Resized image array\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    scale = longest_edge / max(height, width)\n",
    "    image = cv2.resize(image, dsize=(int(width * scale), int(height * scale)), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_image_tensor(image_path_or_array, resize, resize_shape, resize_longest_edge, scale, grayscale):\n",
    "\n",
    "    \"\"\"\n",
    "    Load image and preprocess it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path_or_array: str or numpy.ndarray of shape (height, width, 3)\n",
    "        Image path or image array\n",
    "\n",
    "    resize: bool\n",
    "        Whether to resize the image or not\n",
    "\n",
    "    resize_shape: tuple or int\n",
    "        Tuple of image height and width or number of pixels for both height and width\n",
    "\n",
    "    resize_longest_edge: bool\n",
    "        Whether to resize the longest edge or not\n",
    "\n",
    "    scale: bool\n",
    "        Whether to scale image pixel values by max 8-bit pixel value or not\n",
    "\n",
    "    grayscale: bool\n",
    "        Whether to convert RGB image to grayscale or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image: torch.Tensor of shape (1, 1 or 3, height, width)\n",
    "        Image tensor\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image_path_or_array, pathlib.Path) or isinstance(image_path_or_array, str):\n",
    "        # Read image from the given path if image_path_or_array is a path-like string\n",
    "        image = cv2.imread(str(image_path_or_array))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        image = image_path_or_array\n",
    "\n",
    "    if resize:\n",
    "        if resize_longest_edge:\n",
    "            image = resize_with_aspect_ratio(image=image, longest_edge=resize_shape)\n",
    "        else:\n",
    "            resize_shape = (resize_shape, resize_shape) if isinstance(resize_shape, int) else resize_shape\n",
    "            image = cv2.resize(image, resize_shape, interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    if scale:\n",
    "        image = image / 255.\n",
    "\n",
    "    image = kornia.image_to_tensor(image, False).float()\n",
    "    if grayscale:\n",
    "        image = kornia.color.rgb_to_grayscale(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def crop(image, keypoints, perc_points=0.85, pad=5):\n",
    "    \n",
    "    norm_keypoints = MinMaxScaler().fit_transform(keypoints)\n",
    "    total = len(keypoints)\n",
    "    best_dist = 1\n",
    "    best_clusters = None\n",
    "    best_asm = None\n",
    "    \n",
    "    for eps in [0.01, 0.025, 0.05, 0.1, 0.2]:\n",
    "        clusters = DBSCAN(eps=eps).fit_predict(norm_keypoints)\n",
    "        counts = pd.Series(clusters).value_counts().sort_values(ascending=False)\n",
    "        counts = counts[counts.index > -1]\n",
    "        \n",
    "        if len(counts) == 0:\n",
    "            continue\n",
    "\n",
    "        cumsums = np.cumsum(counts.values) / total\n",
    "        dists = np.abs(cumsums - perc_points)\n",
    "        best_ix = np.argmin(dists)\n",
    "\n",
    "        if dists[best_ix] < best_dist:\n",
    "            best_dist = dists[best_ix]\n",
    "            best_clusters = list(counts.head(best_ix + 1).index)\n",
    "            best_asm = clusters\n",
    "\n",
    "    mask = np.isin(best_asm, best_clusters)\n",
    "\n",
    "    miny = int(np.min(keypoints[mask][:, 1]))\n",
    "    miny = max(miny - pad, 0)\n",
    "\n",
    "    maxy = int(np.max(keypoints[mask][:, 1]))\n",
    "    maxy = min(maxy + pad, image.shape[0])\n",
    "\n",
    "    minx = int(np.min(keypoints[mask][:, 0]))\n",
    "    minx = max(minx - pad, 0)\n",
    "\n",
    "    maxx = int(np.max(keypoints[mask][:, 0]))\n",
    "    maxx = min(maxx + pad, image.shape[1])\n",
    "\n",
    "    image_cropped = image[miny:maxy + 1, minx:maxx + 1, :]\n",
    "    \n",
    "    keypoints_cropped = np.copy(keypoints)\n",
    "    keypoints_cropped[:, 0] -= minx\n",
    "    keypoints_cropped[:, 1] -= miny\n",
    "    keypoints_cropped = keypoints_cropped[\n",
    "        (keypoints_cropped[:, 0] > 0) &\n",
    "        (keypoints_cropped[:, 1] > 0) &\n",
    "        (keypoints_cropped[:, 0] < image_cropped.shape[1]) &\n",
    "        (keypoints_cropped[:, 1] < image_cropped.shape[0])\n",
    "    ]\n",
    "\n",
    "    return image_cropped, keypoints_cropped, minx, miny\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3295be",
   "metadata": {},
   "source": [
    "## 3. Camera Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focal_length(image_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Get focal length from EXIF or calculate it using prior\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path: str\n",
    "        Image path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    focal_length: float\n",
    "        Focal length extracted from EXIF or calculated using prior\n",
    "    \"\"\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image_longest_edge = max(image.size)\n",
    "\n",
    "    focal_length = None\n",
    "    exif = image.getexif()\n",
    "\n",
    "    if exif is not None:\n",
    "\n",
    "        focal_length_35mm = None\n",
    "\n",
    "        for tag, value in exif.items():\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_length_35mm = float(value)\n",
    "\n",
    "        if focal_length_35mm is not None:\n",
    "            focal_length = focal_length_35mm / 35. * image_longest_edge\n",
    "\n",
    "    if focal_length is None:\n",
    "        prior_focal_length = 1.2\n",
    "        focal_length = prior_focal_length * image_longest_edge\n",
    "\n",
    "    return focal_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390295b4",
   "metadata": {},
   "source": [
    "## 4. Database Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7569f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    return array.tobytes()\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_camera(db, image_path, camera_model):\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal_length = get_focal_length(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0\n",
    "        param_arr = np.array([focal_length, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1\n",
    "        param_arr = np.array([focal_length, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2\n",
    "        param_arr = np.array([focal_length, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4\n",
    "        param_arr = np.array([focal_length, focal_length, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, camera_model, single_camera=True):\n",
    "    \n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    \n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        \n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        path = os.path.join(image_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(filename, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    \n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "                added.add(pair_id)\n",
    "                pbar.update(1)\n",
    "                \n",
    "                \n",
    "def get_unique_idxs(A, dim=0):\n",
    "    \n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    \n",
    "    return first_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8b21b",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_string(a):\n",
    "\n",
    "    \"\"\"\n",
    "    Flatten given array and convert it to a string with semicolon delimiters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a: np.ndarray\n",
    "        N-dimensional array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    s: string\n",
    "        String form of the given array\n",
    "    \"\"\"\n",
    "\n",
    "    s = ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def string_to_array(s):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert semicolon delimited string to an array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s: string\n",
    "        String form of the array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a: np.ndarray\n",
    "        N-dimensional array\n",
    "    \"\"\"\n",
    "\n",
    "    a = np.array(s.split(';')).astype(np.float64)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def rotation_matrix_to_quaternion(rotation_matrix):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert rotation matrix to quaternion\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rotation_matrix: numpy.ndarray of shape (3, 3)\n",
    "        Array of directions of the world-axes in camera coordinates\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quaternion: numpy.ndarray of shape (4)\n",
    "        Array of quaternion\n",
    "    \"\"\"\n",
    "\n",
    "    r00 = rotation_matrix[0, 0]\n",
    "    r01 = rotation_matrix[0, 1]\n",
    "    r02 = rotation_matrix[0, 2]\n",
    "    r10 = rotation_matrix[1, 0]\n",
    "    r11 = rotation_matrix[1, 1]\n",
    "    r12 = rotation_matrix[1, 2]\n",
    "    r20 = rotation_matrix[2, 0]\n",
    "    r21 = rotation_matrix[2, 1]\n",
    "    r22 = rotation_matrix[2, 2]\n",
    "\n",
    "    k = np.array([\n",
    "        [r00 - r11 - r22, 0.0, 0.0, 0.0],\n",
    "        [r01 + r10, r11 - r00 - r22, 0.0, 0.0],\n",
    "        [r02 + r20, r12 + r21, r22 - r00 - r11, 0.0],\n",
    "        [r21 - r12, r02 - r20, r10 - r01, r00 + r11 + r22]\n",
    "    ])\n",
    "    k /= 3.0\n",
    "\n",
    "    # Quaternion is the eigenvector of k that corresponds to the largest eigenvalue\n",
    "    w, v = np.linalg.eigh(k)\n",
    "    quaternion = v[[3, 0, 1, 2], np.argmax(w)]\n",
    "\n",
    "    if quaternion[0] < 0:\n",
    "        np.negative(quaternion, quaternion)\n",
    "\n",
    "    return quaternion\n",
    "\n",
    "\n",
    "def pose_difference(r1, t1, r2, t2):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate relative pose difference from given rotation matrices and translation vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r1: numpy.ndarray of shape (3, 3)\n",
    "        First rotation matrix\n",
    "\n",
    "    t1: numpy.ndarray of shape (3)\n",
    "        First translation vector\n",
    "\n",
    "    r2: numpy.ndarray of shape (3, 3)\n",
    "        Second rotation matrix\n",
    "\n",
    "    t2: numpy.ndarray of shape (3)\n",
    "        Second translation vector\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rotation_difference: float\n",
    "        Rotation difference in terms of degrees from the first image\n",
    "\n",
    "    translation_difference: float\n",
    "        Translation difference in terms of meters from the first image\n",
    "    \"\"\"\n",
    "\n",
    "    rotation_difference = np.dot(r2, r1.T)\n",
    "    translation_difference = t2 - np.dot(rotation_difference, t1)\n",
    "\n",
    "    return rotation_difference, translation_difference\n",
    "\n",
    "\n",
    "def rotation_and_translation_error(q_ground_truth, t_ground_truth, q_prediction, t_prediction, epsilon=1e-15):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate rotation and translation error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_ground_truth: numpy.ndarray of shape (4)\n",
    "        Array of quaternion derived from ground truth rotation matrix\n",
    "\n",
    "    t_ground_truth: numpy.ndarray of shape (3)\n",
    "        Array of ground truth translation vector\n",
    "\n",
    "    q_prediction: numpy.ndarray of shape (4)\n",
    "        Array of quaternion derived from estimated rotation matrix\n",
    "\n",
    "    t_prediction: numpy.ndarray of shape (3)\n",
    "        Array of estimated translation vector\n",
    "\n",
    "    epsilon: float\n",
    "        A small number for preventing zero division\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rotation_error: float\n",
    "        Rotation error in terms of degrees\n",
    "\n",
    "    translation_error: float\n",
    "        Translation error in terms of meters\n",
    "    \"\"\"\n",
    "\n",
    "    q_ground_truth_norm = q_ground_truth / (np.linalg.norm(q_ground_truth) + epsilon)\n",
    "    q_prediction_norm = q_prediction / (np.linalg.norm(q_prediction) + epsilon)\n",
    "    loss_q = np.maximum(epsilon, (1.0 - np.sum(q_prediction_norm * q_ground_truth_norm) ** 2))\n",
    "\n",
    "    rotation_error = np.degrees(np.arccos(1 - (2 * loss_q)))\n",
    "\n",
    "    scaling_factor = np.linalg.norm(t_ground_truth)\n",
    "    t_prediction = scaling_factor * (t_prediction / (np.linalg.norm(t_prediction) + epsilon))\n",
    "    translation_error = min(\n",
    "        np.linalg.norm(t_ground_truth - t_prediction),\n",
    "        np.linalg.norm(t_ground_truth + t_prediction)\n",
    "    )\n",
    "\n",
    "    return rotation_error, translation_error\n",
    "\n",
    "\n",
    "def mean_average_accuracy(rotation_errors, translation_errors, rotation_error_thresholds, translation_error_thresholds):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate mean average accuracies over a set of thresholds for rotation and translation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rotation_errors: list of shape (n_pairs)\n",
    "        List of rotation errors\n",
    "\n",
    "    translation_errors: list of shape (n_pairs)\n",
    "        List of translation errors\n",
    "\n",
    "    rotation_error_thresholds: numpy.ndarray of shape (10)\n",
    "        Array of rotation error thresholds\n",
    "\n",
    "    translation_error_thresholds: numpy.ndarray of shape (10)\n",
    "        Array of translation error thresholds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    maa: float\n",
    "        Mean average accuracy calculated on both rotation and translation errors\n",
    "\n",
    "    rotation_maa: float\n",
    "        Mean average accuracy calculated on rotation errors\n",
    "\n",
    "    translation_maa: float\n",
    "        Mean average accuracy calculated on translation errors\n",
    "    \"\"\"\n",
    "\n",
    "    accuracies, rotation_accuracies, translation_accuracies = [], [], []\n",
    "\n",
    "    for rotation_error_threshold, translation_error_threshold in zip(rotation_error_thresholds, translation_error_thresholds):\n",
    "\n",
    "        # Calculate whether the errors are less than specified thresholds or not\n",
    "        rotation_accuracy = (rotation_errors <= rotation_error_threshold)\n",
    "        translation_accuracy = (translation_errors <= translation_error_threshold)\n",
    "        accuracy = rotation_accuracy & translation_accuracy\n",
    "\n",
    "        accuracies.append(accuracy.astype(np.float32).mean())\n",
    "        rotation_accuracies.append(rotation_accuracy.astype(np.float32).mean())\n",
    "        translation_accuracies.append(translation_accuracy.astype(np.float32).mean())\n",
    "\n",
    "    maa = np.array(accuracies).mean()\n",
    "    rotation_maa = np.array(rotation_accuracies).mean()\n",
    "    translation_maa = np.array(translation_accuracies).mean()\n",
    "\n",
    "    return maa, rotation_maa, translation_maa\n",
    "\n",
    "\n",
    "def evaluate(df, verbose=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate mean average accuracies over a set of thresholds for rotation and translation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe with dataset, scene, rotation_matrix, translation_vector, rotation_matrix_prediction and translation_vector_prediction columns\n",
    "\n",
    "    verbose: bool\n",
    "        Whether to print scores or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_scores: pandas.DataFrame\n",
    "        Dataframe of scores\n",
    "    \"\"\"\n",
    "\n",
    "    rotation_error_thresholds = {\n",
    "        **{('haiper', scene): np.linspace(1, 10, 10) for scene in ['bike', 'chairs', 'fountain']},\n",
    "        **{('heritage', scene): np.linspace(1, 10, 10) for scene in ['cyprus', 'dioscuri']},\n",
    "        **{('heritage', 'wall'): np.linspace(0.2, 10, 10)},\n",
    "        **{('urban', 'kyiv-puppet-theater'): np.linspace(1, 10, 10)},\n",
    "    }\n",
    "    translation_error_thresholds = {\n",
    "        **{('haiper', scene): np.geomspace(0.05, 0.5, 10) for scene in ['bike', 'chairs', 'fountain']},\n",
    "        **{('heritage', scene): np.geomspace(0.1, 2, 10) for scene in ['cyprus', 'dioscuri']},\n",
    "        **{('heritage', 'wall'): np.geomspace(0.05, 1, 10)},\n",
    "        **{('urban', 'kyiv-puppet-theater'): np.geomspace(0.5, 5, 10)},\n",
    "    }\n",
    "    df_scores = pd.DataFrame(columns=['dataset', 'scene', 'image_pairs', 'maa', 'rotation_maa', 'translation_maa'])\n",
    "\n",
    "    for (dataset, scene), df_scene in tqdm(df.groupby(['dataset', 'scene'])):\n",
    "\n",
    "        scene_rotation_errors = []\n",
    "        scene_translation_errors = []\n",
    "\n",
    "        for i in range(df_scene.shape[0]):\n",
    "            for j in range(i + 1, df_scene.shape[0]):\n",
    "\n",
    "                rotation_matrix_difference_ground_truth, translation_vector_difference_ground_truth = pose_difference(\n",
    "                    r1=string_to_array((df_scene.iloc[i]['rotation_matrix'])).reshape(3, 3),\n",
    "                    t1=string_to_array((df_scene.iloc[i]['translation_vector'])),\n",
    "                    r2=string_to_array((df_scene.iloc[j]['rotation_matrix'])).reshape(3, 3),\n",
    "                    t2=string_to_array((df_scene.iloc[j]['translation_vector'])),\n",
    "                )\n",
    "                quaternion_ground_truth = rotation_matrix_to_quaternion(rotation_matrix=rotation_matrix_difference_ground_truth)\n",
    "\n",
    "                rotation_matrix_difference_prediction, translation_vector_difference_prediction = pose_difference(\n",
    "                    r1=string_to_array((df_scene.iloc[i]['rotation_matrix_prediction'])).reshape(3, 3),\n",
    "                    t1=string_to_array((df_scene.iloc[i]['translation_vector_prediction'])),\n",
    "                    r2=string_to_array((df_scene.iloc[j]['rotation_matrix_prediction'])).reshape(3, 3),\n",
    "                    t2=string_to_array((df_scene.iloc[j]['translation_vector_prediction'])),\n",
    "                )\n",
    "                quaternion_prediction = rotation_matrix_to_quaternion(rotation_matrix=rotation_matrix_difference_prediction)\n",
    "\n",
    "                rotation_error, translation_error = rotation_and_translation_error(\n",
    "                    q_ground_truth=quaternion_ground_truth,\n",
    "                    t_ground_truth=translation_vector_difference_ground_truth,\n",
    "                    q_prediction=quaternion_prediction,\n",
    "                    t_prediction=translation_vector_difference_prediction,\n",
    "                    epsilon=1e-15\n",
    "                )\n",
    "                scene_rotation_errors.append(rotation_error)\n",
    "                scene_translation_errors.append(translation_error)\n",
    "\n",
    "        scene_maa, scene_rotation_maa, scene_translation_maa = mean_average_accuracy(\n",
    "            rotation_errors=scene_rotation_errors,\n",
    "            translation_errors=scene_translation_errors,\n",
    "            rotation_error_thresholds=rotation_error_thresholds[(dataset, scene)],\n",
    "            translation_error_thresholds=translation_error_thresholds[(dataset, scene)]\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            settings.logger.info(\n",
    "                f'''\n",
    "                Dataset: {dataset} - Scene: {scene}\n",
    "                Number of image pairs: {len(scene_rotation_errors)}\n",
    "                mAA: {scene_maa:.6f} - rotation mAA: {scene_rotation_maa:.6f} - translation mAA: {scene_translation_maa:.6f}\n",
    "                '''\n",
    "            )\n",
    "\n",
    "        df_scores = pd.concat((\n",
    "            df_scores,\n",
    "            pd.DataFrame(\n",
    "                data=[[dataset, scene, len(scene_rotation_errors), scene_maa, scene_rotation_maa, scene_translation_maa]],\n",
    "                columns=['dataset', 'scene', 'image_pairs', 'maa', 'rotation_maa', 'translation_maa']\n",
    "            )\n",
    "        ), axis=0)\n",
    "\n",
    "    return df_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71073a8",
   "metadata": {},
   "source": [
    "## 6. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff116369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loftr_match_images(image1, image2, model, device, amp, transforms, confidence_threshold, top_k):\n",
    "\n",
    "    \"\"\"\n",
    "    Match given two images with each other using LoFTR model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image1: numpy.ndarray of shape (3, height, width)\n",
    "        Array of first image\n",
    "\n",
    "    image2: numpy.ndarray of shape (3, height, width)\n",
    "        Array of second image\n",
    "\n",
    "    model: torch.nn.Module\n",
    "        LoFTR Model\n",
    "\n",
    "    device: torch.device\n",
    "        Location of the image1, image2 and the model\n",
    "\n",
    "    amp: bool\n",
    "        Whether to use auto mixed precision or not\n",
    "\n",
    "    transforms: dict\n",
    "        Dictionary of transform parameters\n",
    "\n",
    "    confidence_threshold: float or int\n",
    "        Confidence threshold to filter out low confidence matches\n",
    "\n",
    "    top_k: int\n",
    "        Number of matches to take\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs: dict\n",
    "        Model outputs\n",
    "    \"\"\"\n",
    "\n",
    "    image1_raw_height, image1_raw_width = image1.shape[:2]\n",
    "    image1 = get_image_tensor(\n",
    "        image_path_or_array=image1,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image1 = image1.to(device)\n",
    "    image1_transformed_height, image1_transformed_width = image1.shape[2:]\n",
    "\n",
    "    image2_raw_height, image2_raw_width = image2.shape[:2]\n",
    "    image2 = get_image_tensor(\n",
    "        image_path_or_array=image2,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image2 = image2.to(device)\n",
    "    image2_transformed_height, image2_transformed_width = image2.shape[2:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if amp:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                outputs = model({'image0': image1, 'image1': image2})\n",
    "        else:\n",
    "            outputs = model({'image0': image1, 'image1': image2})\n",
    "\n",
    "    for k in outputs.keys():\n",
    "        outputs[k] = outputs[k].detach().cpu().numpy()\n",
    "\n",
    "    if confidence_threshold is not None:\n",
    "        if isinstance(confidence_threshold, float):\n",
    "            # Select matched keypoints with above given confidence threshold\n",
    "            confidence_mask = outputs['confidence'] >= confidence_threshold\n",
    "        elif isinstance(confidence_threshold, int):\n",
    "            # Select keypoints dynamically based on confidence distribution\n",
    "            confidence_mean, confidence_std = outputs['confidence'].mean(), outputs['confidence'].std()\n",
    "            confidence_mask = outputs['confidence'] >= (confidence_mean + (confidence_std * confidence_threshold))\n",
    "        else:\n",
    "            raise ValueError(f'Invalid confidence_threshold {confidence_threshold}')\n",
    "\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][confidence_mask]\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Select top-k keypoints based on their confidences\n",
    "        sorting_idx = outputs['matching_scores0'].argsort()[-top_k:]\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][sorting_idx]\n",
    "\n",
    "    outputs['keypoints0'][:, 0] *= image1_raw_width / image1_transformed_width\n",
    "    outputs['keypoints0'][:, 1] *= image1_raw_height / image1_transformed_height\n",
    "    outputs['keypoints1'][:, 0] *= image2_raw_width / image2_transformed_width\n",
    "    outputs['keypoints1'][:, 1] *= image2_raw_height / image2_transformed_height\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a313aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superglue_match_images(image1, image2, model, device, amp, transforms, score_threshold, top_k):\n",
    "\n",
    "    \"\"\"\n",
    "    Match given two images with each other using SuperGlue model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image1: numpy.ndarray of shape (3, height, width)\n",
    "        Array of first image\n",
    "\n",
    "    image2: numpy.ndarray of shape (3, height, width)\n",
    "        Array of second image\n",
    "\n",
    "    model: torch.nn.Module\n",
    "        SuperGlue Model\n",
    "\n",
    "    device: torch.device\n",
    "        Location of the image1, image2 and the model\n",
    "\n",
    "    amp: bool\n",
    "        Whether to use auto mixed precision or not\n",
    "\n",
    "    transforms: dict\n",
    "        Dictionary of transform parameters\n",
    "\n",
    "    score_threshold: float, int or None\n",
    "        Confidence threshold\n",
    "\n",
    "    top_k: int or None\n",
    "        Number of keypoints to take\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs: dict\n",
    "        Model outputs\n",
    "    \"\"\"\n",
    "\n",
    "    image1_raw_height, image1_raw_width = image1.shape[:2]\n",
    "    image1 = get_image_tensor(\n",
    "        image_path_or_array=image1,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image1 = image1.to(device)\n",
    "    image1_transformed_height, image1_transformed_width = image1.shape[2:]\n",
    "\n",
    "    image2_raw_height, image2_raw_width = image2.shape[:2]\n",
    "    image2 = get_image_tensor(\n",
    "        image_path_or_array=image2,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image2 = image2.to(device)\n",
    "    image2_transformed_height, image2_transformed_width = image2.shape[2:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if amp:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "                outputs = model({'image0': image1, 'image1': image2})\n",
    "        else:\n",
    "            outputs = model({'image0': image1, 'image1': image2})\n",
    "\n",
    "    for k in outputs.keys():\n",
    "        if k == 'descriptors0' or k == 'descriptors1':\n",
    "            outputs[k] = outputs[k][0].detach().cpu().numpy().T\n",
    "        else:\n",
    "            outputs[k] = outputs[k][0].detach().cpu().numpy()\n",
    "\n",
    "    matches_mask = outputs['matches0'] > -1\n",
    "\n",
    "    for k in ['keypoints1', 'scores1', 'descriptors1', 'matches1', 'matching_scores1']:\n",
    "        outputs[k] = outputs[k][outputs['matches0'][matches_mask]]\n",
    "\n",
    "    for k in ['keypoints0', 'scores0', 'descriptors0', 'matches0', 'matching_scores0']:\n",
    "        outputs[k] = outputs[k][matches_mask]\n",
    "\n",
    "    if score_threshold is not None:\n",
    "        if isinstance(score_threshold, float):\n",
    "            # Select matched keypoints with above given score threshold\n",
    "            score_mask = outputs['matching_scores0'] >= score_threshold\n",
    "        elif isinstance(score_threshold, int):\n",
    "            # Select keypoints dynamically based on score distribution\n",
    "            score_mean, score_std = outputs['matching_scores0'].mean(), outputs['matching_scores0'].std()\n",
    "            score_mask = outputs['matching_scores0'] >= (score_mean + (score_std * score_threshold))\n",
    "        else:\n",
    "            raise ValueError(f'Invalid score_threshold {score_threshold}')\n",
    "\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][score_mask]\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Select top-k keypoints based on their scores\n",
    "        sorting_idx = outputs['matching_scores0'].argsort()[-top_k:]\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][sorting_idx]\n",
    "\n",
    "    outputs['keypoints0'][:, 0] *= image1_raw_width / image1_transformed_width\n",
    "    outputs['keypoints0'][:, 1] *= image1_raw_height / image1_transformed_height\n",
    "    outputs['keypoints1'][:, 0] *= image2_raw_width / image2_transformed_width\n",
    "    outputs['keypoints1'][:, 1] *= image2_raw_height / image2_transformed_height\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b341aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalFeatureDetectorDescriptor(LocalFeature):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            orientation_module_name, orientation_module_parameters, orientation_module_weights_path,\n",
    "            affine_module_name, affine_module_parameters, affine_module_weights_path,\n",
    "            detector_module_name, detector_module_parameters, detector_module_weights_path,\n",
    "            descriptor_module_name, descriptor_module_parameters, descriptor_module_weights_path,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Module that combines local feature detector and descriptor\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        orientation_module_name: str\n",
    "            Name of the orientation module\n",
    "\n",
    "        orientation_module_parameters: dict\n",
    "            Parameters of the orientation module\n",
    "\n",
    "        orientation_module_weights_path: str\n",
    "            Path of the orientation module weights\n",
    "\n",
    "        affine_module_name: str\n",
    "            Name of the affine module\n",
    "\n",
    "        affine_module_parameters: dict\n",
    "            Parameters of the affine module\n",
    "\n",
    "        affine_module_weights_path: str\n",
    "            Path of the affine module weights\n",
    "\n",
    "        detector_module_name: str\n",
    "            Name of the detector module\n",
    "\n",
    "        detector_module_parameters: dict\n",
    "            Parameters of the detector module\n",
    "\n",
    "        detector_module_weights_path: str\n",
    "            Path of the detector module weights\n",
    "\n",
    "        descriptor_module_name: str\n",
    "            Name of the descriptor module\n",
    "\n",
    "        descriptor_module_parameters: dict\n",
    "            Parameters of the descriptor module\n",
    "\n",
    "        descriptor_module_weights_path: str\n",
    "            Path of the descriptor module weights\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lafs: torch.Tensor of shape (1, n_detections, 2, 3)\n",
    "            Detected local affine frames\n",
    "\n",
    "        responses: torch.Tensor of shape (1, n_detections)\n",
    "            Response function values for corresponding lafs\n",
    "\n",
    "        descriptors: torch.Tensor of shape (1, n_detections, n_dimensions)\n",
    "            Local descriptors\n",
    "        \"\"\"\n",
    "\n",
    "        # Instantiate specified orientation module\n",
    "        if orientation_module_name == 'PassLAF':\n",
    "            orientation_module = PassLAF()\n",
    "        elif orientation_module_name == 'OriNet':\n",
    "            orientation_module = LAFOrienter(**orientation_module_parameters, angle_detector=OriNet(pretrained=(orientation_module_weights_path is None)))\n",
    "        elif orientation_module_name == 'PatchDominantGradientOrientation':\n",
    "            orientation_module = LAFOrienter(**orientation_module_parameters, angle_detector=PatchDominantGradientOrientation())\n",
    "        else:\n",
    "            orientation_module = None\n",
    "\n",
    "        # Instantiate specified affine module\n",
    "        if affine_module_name == 'LAFAffNetShapeEstimator':\n",
    "            affine_module = LAFAffNetShapeEstimator(**affine_module_parameters, pretrained=(affine_module_weights_path is None))\n",
    "        else:\n",
    "            affine_module = None\n",
    "\n",
    "        # Instantiate specified detector module\n",
    "        if detector_module_name == 'KeyNetDetector':\n",
    "            detector_module = KeyNetDetector(**detector_module_parameters, pretrained=(detector_module_weights_path is None), ori_module=orientation_module, aff_module=affine_module)\n",
    "        else:\n",
    "            detector_module = None\n",
    "\n",
    "        # Load pretrained weights for the detector module\n",
    "        if orientation_module_weights_path is not None:\n",
    "            detector_module.ori.angle_detector.load_state_dict(torch.load(orientation_module_weights_path)['state_dict'])\n",
    "        if affine_module_weights_path is not None:\n",
    "            detector_module.aff.load_state_dict(torch.load(affine_module_weights_path)['state_dict'])\n",
    "        if detector_module_weights_path is not None:\n",
    "            detector_module.model.load_state_dict(torch.load(detector_module_weights_path)['state_dict'])\n",
    "\n",
    "        # Instantiate specified descriptor module\n",
    "        if descriptor_module_name == 'HardNet8':\n",
    "            descriptor_module = LAFDescriptor(**descriptor_module_parameters, patch_descriptor_module=HardNet8(pretrained=(descriptor_module_weights_path is None)))\n",
    "        elif descriptor_module_name == 'HyNet':\n",
    "            descriptor_module = LAFDescriptor(**descriptor_module_parameters, patch_descriptor_module=HyNet(pretrained=(descriptor_module_weights_path is None)))\n",
    "        elif descriptor_module_name == 'TFeat':\n",
    "            descriptor_module = LAFDescriptor(**descriptor_module_parameters, patch_descriptor_module=TFeat(pretrained=(descriptor_module_weights_path is None)))\n",
    "        elif descriptor_module_name == 'SOSNet':\n",
    "            descriptor_module = LAFDescriptor(**descriptor_module_parameters, patch_descriptor_module=SOSNet(pretrained=(descriptor_module_weights_path is None)))\n",
    "        else:\n",
    "            descriptor_module = None\n",
    "\n",
    "        # Load pretrained weights for the descriptor module\n",
    "        if descriptor_module_weights_path is not None:\n",
    "            descriptor_module.descriptor.load_state_dict(torch.load(descriptor_module_weights_path))\n",
    "\n",
    "        super().__init__(detector_module, descriptor_module)\n",
    "\n",
    "\n",
    "def extract_keypoints_and_descriptors(image, model, amp, device):\n",
    "\n",
    "    \"\"\"\n",
    "    Extract local feature descriptors on given image with given model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: torch.Tensor of shape (1, 1, height, width)\n",
    "        Image tensor\n",
    "\n",
    "    model: torch.nn.Module\n",
    "        Local feature detector and descriptor model\n",
    "\n",
    "    device: torch.device\n",
    "        Location of the image1, image2 and the model\n",
    "\n",
    "    amp: bool\n",
    "        Whether to use auto mixed precision or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lafs: numpy.ndarray of shape (n_detections, 2, 3)\n",
    "        Detected local affine frames\n",
    "\n",
    "    responses: numpy.ndarray of shape (n_detections)\n",
    "        Response function values for corresponding lafs\n",
    "\n",
    "    descriptors: numpy.ndarray of shape (n_detections, n_dimensions)\n",
    "        Local descriptors\n",
    "\n",
    "    keypoints: numpy.ndarray of shape (n_detections, 2)\n",
    "        Keypoints\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if amp:\n",
    "            with torch.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                lafs, responses, descriptors = model(image)\n",
    "        else:\n",
    "            lafs, responses, descriptors = model(image)\n",
    "\n",
    "    responses = torch.squeeze(responses, dim=0).detach().cpu().numpy()\n",
    "    descriptors = torch.squeeze(descriptors, dim=0).detach().cpu()\n",
    "    keypoints = get_laf_center(lafs)\n",
    "    keypoints = keypoints.detach().cpu().numpy().reshape(-1, 2)\n",
    "    lafs = lafs.detach().cpu().numpy()\n",
    "\n",
    "    return lafs, responses, descriptors, keypoints\n",
    "\n",
    "\n",
    "def match_descriptors(descriptors1, descriptors2, matcher):\n",
    "\n",
    "    \"\"\"\n",
    "    Match descriptors with nearest neighbor algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    descriptors1: torch.Tensor of shape (n_detections, n_dimensions)\n",
    "        Descriptors from first image\n",
    "\n",
    "    descriptors2: torch.Tensor of shape (n_detections, n_dimensions):\n",
    "        Descriptors from second image\n",
    "\n",
    "    matcher: torch.nn.Module\n",
    "        Descriptor matcher\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distances: numpy.ndarray of shape (n_matches)\n",
    "        Distances of matching descriptors\n",
    "\n",
    "    indexes: numpy.ndarray of shape (n_matches, 2)\n",
    "        Indexes of matching descriptors\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        distances, indexes = matcher(descriptors1, descriptors2)\n",
    "\n",
    "    distances = distances.detach().cpu().numpy().reshape(-1)\n",
    "    indexes = indexes.detach().cpu().numpy()\n",
    "\n",
    "    return distances, indexes\n",
    "\n",
    "\n",
    "def lfdd_match_images(image1, image2, model, matcher, device, amp, transforms, distance_threshold, top_k):\n",
    "\n",
    "    \"\"\"\n",
    "    Match given two images with each other using given model and matcher\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image1: numpy.ndarray of shape (3, height, width)\n",
    "        Array of first image\n",
    "\n",
    "    image2: numpy.ndarray of shape (3, height, width)\n",
    "        Array of second image\n",
    "\n",
    "    model: torch.nn.Module\n",
    "        Local feature detector and descriptor model\n",
    "\n",
    "    matcher: torch.nn.Module\n",
    "        Descriptor matcher\n",
    "\n",
    "    device: torch.device\n",
    "        Location of the image1, image2 and the model\n",
    "\n",
    "    amp: bool\n",
    "        Whether to use auto mixed precision or not\n",
    "\n",
    "    transforms: dict\n",
    "        Dictionary of transform parameters\n",
    "\n",
    "    distance_threshold: float\n",
    "        Threshold to filter out keypoints with low distance\n",
    "\n",
    "    top_k: int\n",
    "        Number of keypoints to take\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs: dict\n",
    "        Model outputs\n",
    "    \"\"\"\n",
    "\n",
    "    image1_raw_height, image1_raw_width = image1.shape[:2]\n",
    "    image1 = get_image_tensor(\n",
    "        image_path_or_array=image1,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image1 = image1.to(device)\n",
    "    image1_transformed_height, image1_transformed_width = image1.shape[2:]\n",
    "\n",
    "    image2_raw_height, image2_raw_width = image2.shape[:2]\n",
    "    image2 = get_image_tensor(\n",
    "        image_path_or_array=image2,\n",
    "        resize=transforms['resize'],\n",
    "        resize_shape=transforms['resize_shape'],\n",
    "        resize_longest_edge=transforms['resize_longest_edge'],\n",
    "        scale=transforms['scale'],\n",
    "        grayscale=transforms['grayscale']\n",
    "    )\n",
    "    image2 = image2.to(device)\n",
    "    image2_transformed_height, image2_transformed_width = image2.shape[2:]\n",
    "\n",
    "    _, _, descriptors1, keypoints1 = extract_keypoints_and_descriptors(image=image1, model=model, amp=amp, device=device)\n",
    "    _, _, descriptors2, keypoints2 = extract_keypoints_and_descriptors(image=image2, model=model, amp=amp, device=device)\n",
    "    distances, indexes = match_descriptors(descriptors1=descriptors1, descriptors2=descriptors2, matcher=matcher)\n",
    "\n",
    "    outputs = {\n",
    "        'keypoints0': keypoints1[indexes[:, 0]],\n",
    "        'keypoints1': keypoints2[indexes[:, 1]],\n",
    "        'distances': distances\n",
    "    }\n",
    "\n",
    "    if distance_threshold is not None:\n",
    "        if isinstance(distance_threshold, float):\n",
    "            # Select matched keypoints with above given distance threshold\n",
    "            distance_mask = outputs['distances'] >= distance_threshold\n",
    "        elif isinstance(distance_threshold, int):\n",
    "            # Select keypoints dynamically based on distance distribution\n",
    "            distance_mean, distance_std = outputs['distances'].mean(), outputs['distances'].std()\n",
    "            distance_mask = outputs['distances'] >= (distance_mean + (distance_std * distance_threshold))\n",
    "        else:\n",
    "            raise ValueError(f'Invalid distance_threshold {distance_threshold}')\n",
    "\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][distance_mask]\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Select top-k keypoints based on their distances\n",
    "        sorting_idx = outputs['distances'].argsort()[-top_k:]\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][sorting_idx]\n",
    "\n",
    "    outputs['keypoints0'][:, 0] *= image1_raw_width / image1_transformed_width\n",
    "    outputs['keypoints0'][:, 1] *= image1_raw_height / image1_transformed_height\n",
    "    outputs['keypoints1'][:, 0] *= image2_raw_width / image2_transformed_width\n",
    "    outputs['keypoints1'][:, 1] *= image2_raw_height / image2_transformed_height\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b614694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift_match_images(image1, image2, model, matcher, transforms, distance_threshold, top_k):\n",
    "\n",
    "    image1_raw_height, image1_raw_width = image1.shape[:2]\n",
    "    if transforms['resize']:\n",
    "        image1 = resize_with_aspect_ratio(\n",
    "            image=image1,\n",
    "            longest_edge=transforms['resize_shape']\n",
    "        )\n",
    "    image1_transformed_height, image1_transformed_width = image1.shape[:2]\n",
    "\n",
    "    image2_raw_height, image2_raw_width = image2.shape[:2]\n",
    "    if transforms['resize']:\n",
    "        image2 = resize_with_aspect_ratio(\n",
    "            image=image2,\n",
    "            longest_edge=transforms['resize_shape']\n",
    "        )\n",
    "    image2_transformed_height, image2_transformed_width = image2.shape[:2]\n",
    "        \n",
    "    keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "    matches = matcher.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    outputs = {\n",
    "        'keypoints0': np.array([keypoints1[match.queryIdx].pt for match in matches]),\n",
    "        'keypoints1': np.array([keypoints2[match.trainIdx].pt for match in matches]),\n",
    "        'distances': np.array([match.distance for match in matches]),\n",
    "    }\n",
    "    \n",
    "    if distance_threshold is not None:\n",
    "        if isinstance(distance_threshold, float):\n",
    "            # Select matched keypoints with above given distance threshold\n",
    "            distance_mask = outputs['distances'] <= distance_threshold\n",
    "        elif isinstance(distance_threshold, int):\n",
    "            # Select keypoints dynamically based on distance distribution\n",
    "            distance_mean, distance_std = outputs['distances'].mean(), outputs['distances'].std()\n",
    "            distance_mask = outputs['distances'] <= (distance_mean + (distance_std * distance_threshold))\n",
    "        else:\n",
    "            raise ValueError(f'Invalid distance_threshold {distance_threshold}')\n",
    "\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][distance_mask]\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Select top-k keypoints based on their distances\n",
    "        sorting_idx = outputs['distances'].argsort()\n",
    "        for k in outputs.keys():\n",
    "            outputs[k] = outputs[k][sorting_idx]\n",
    "    \n",
    "    outputs['keypoints0'][:, 0] *= image1_raw_width / image1_transformed_width\n",
    "    outputs['keypoints0'][:, 1] *= image1_raw_height / image1_transformed_height\n",
    "    outputs['keypoints1'][:, 0] *= image2_raw_width / image2_transformed_width\n",
    "    outputs['keypoints1'][:, 1] *= image2_raw_height / image2_transformed_height\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3b08e",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_matching_device = torch.device('cuda')\n",
    "\n",
    "# Load LoFTR model with specified configurations\n",
    "loftr_model = LoFTR(\n",
    "    pretrained=None,\n",
    "    config={\n",
    "        'backbone_type': 'ResNetFPN',\n",
    "        'resolution': (8, 2),\n",
    "        'fine_window_size': 5,\n",
    "        'fine_concat_coarse_feat': True,\n",
    "        'resnetfpn': {\n",
    "            'initial_dim': 128,\n",
    "            'block_dims': [128, 196, 256]\n",
    "        },\n",
    "        'coarse': {\n",
    "            'd_model': 256,\n",
    "            'd_ffn': 256,\n",
    "            'nhead': 8,\n",
    "            'layer_names': ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross'],\n",
    "            'attention': 'linear',\n",
    "            'temp_bug_fix': False,\n",
    "        },\n",
    "        'match_coarse': {\n",
    "            'thr': 0.2,\n",
    "            'border_rm': 2,\n",
    "            'match_type': 'dual_softmax',\n",
    "            'dsmax_temperature': 0.1,\n",
    "            'skh_iters': 3,\n",
    "            'skh_init_bin_score': 1.0,\n",
    "            'skh_prefilter': True,\n",
    "            'train_coarse_percent': 0.4,\n",
    "            'train_pad_num_gt_min': 200,\n",
    "        },\n",
    "        'fine': {\n",
    "            'd_model':128,\n",
    "            'd_ffn': 128,\n",
    "            'nhead': 8,\n",
    "            'layer_names': ['self', 'cross'],\n",
    "            'attention': 'linear'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "loftr_model.load_state_dict(torch.load(root / 'models' / 'loftr' / 'loftr_outdoor.ckpt')['state_dict'], strict=False)\n",
    "loftr_model = loftr_model.eval().to(image_matching_device)\n",
    "\n",
    "# Load SuperPoint and SuperGlue model with specified configurations\n",
    "superglue_model = Matching(config={\n",
    "    'superpoint': {\n",
    "        'descriptor_dim': 256,\n",
    "        'nms_radius': 4,\n",
    "        'keypoint_threshold': 0.01,\n",
    "        'max_keypoints': -1,\n",
    "        'remove_borders': 4\n",
    "    },\n",
    "    'superglue': {\n",
    "        'descriptor_dim': 256,\n",
    "        'weights': 'outdoor',\n",
    "        'keypoint_encoder': [32, 64, 128, 256],\n",
    "        'sinkhorn_iterations': 100,\n",
    "        'match_threshold': 0.2\n",
    "    }\n",
    "})\n",
    "superglue_model = superglue_model.eval().to(image_matching_device)\n",
    "\n",
    "# Load LFDD model with specified configurations\n",
    "lfdd_model = LocalFeatureDetectorDescriptor(\n",
    "    orientation_module_name='OriNet',\n",
    "    orientation_module_parameters={},\n",
    "    orientation_module_weights_path=None,\n",
    "    affine_module_name='LAFAffNetShapeEstimator',\n",
    "    affine_module_parameters={\n",
    "        'preserve_orientation': True\n",
    "    },\n",
    "    affine_module_weights_path=str(root / 'models' / 'lfdd' / 'affnet.pth'),\n",
    "    detector_module_name='KeyNetDetector',\n",
    "    detector_module_parameters={\n",
    "        'num_features': 2048\n",
    "    },\n",
    "    detector_module_weights_path=str(root / 'models' / 'lfdd' / 'keynet.pth'),\n",
    "    descriptor_module_name='HardNet8',\n",
    "    descriptor_module_parameters={},\n",
    "    descriptor_module_weights_path=str(root / 'models' / 'lfdd' / 'hardnet8v2.pt')\n",
    ")\n",
    "lfdd_model = lfdd_model.eval().to(image_matching_device)\n",
    "descriptor_matcher = DescriptorMatcher(match_mode='snn', th=0.9)\n",
    "\n",
    "# Load SIFT extractor\n",
    "sift = cv2.SIFT_create()\n",
    "bf_matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef518a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(read_image_function, image_paths, image_pair_indices, feature_dir, loftr_model, superglue_model):\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n",
    "        for pair_idx in progress_bar(image_pair_indices):\n",
    "            \n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = image_paths[idx1], image_paths[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            image1 = read_image_function(fname1)\n",
    "            image2 = read_image_function(fname2)\n",
    "            \n",
    "            sift_outputs = sift_match_images(\n",
    "                image1=image1,\n",
    "                image2=image2,\n",
    "                model=sift,\n",
    "                matcher=bf_matcher,\n",
    "                transforms={\n",
    "                    'resize': True,\n",
    "                    'resize_shape': 2000,\n",
    "                    'resize_longest_edge': True,\n",
    "                    'scale': False,\n",
    "                    'grayscale': True,\n",
    "                },\n",
    "                distance_threshold=None,\n",
    "                top_k=None\n",
    "            )\n",
    "            \n",
    "            image_pair_longest_edge = np.concatenate([image1.shape[:2], image2.shape[:2]]).max()\n",
    "            if image_pair_longest_edge > 2000:\n",
    "                superglue_transforms = {\n",
    "                    'resize': True,\n",
    "                    'resize_shape': 1920,\n",
    "                    'resize_longest_edge': True,\n",
    "                    'scale': True,\n",
    "                    'grayscale': True,\n",
    "                }\n",
    "            else:\n",
    "                superglue_transforms = {\n",
    "                    'resize': False,\n",
    "                    'resize_shape': None,\n",
    "                    'resize_longest_edge': None,\n",
    "                    'scale': True,\n",
    "                    'grayscale': True,\n",
    "                }\n",
    "\n",
    "            \n",
    "            '''\n",
    "            loftr_outputs = loftr_match_images(\n",
    "                image1=image1,\n",
    "                image2=image2,\n",
    "                model=loftr_model,\n",
    "                device=torch.device('cuda'),\n",
    "                amp=False,\n",
    "                transforms={\n",
    "                    'resize': True,\n",
    "                    'resize_shape': 840,\n",
    "                    'resize_longest_edge': True,\n",
    "                    'scale': True,\n",
    "                    'grayscale': True,\n",
    "                },\n",
    "                confidence_threshold=None,\n",
    "                top_k=None\n",
    "            )\n",
    "            '''\n",
    "            '''\n",
    "            lfdd_outputs = lfdd_match_images(\n",
    "                image1=image1,\n",
    "                image2=image2,\n",
    "                model=lfdd_model,\n",
    "                matcher=descriptor_matcher,\n",
    "                device=torch.device('cuda'),\n",
    "                amp=False,\n",
    "                transforms={\n",
    "                    'resize': True,\n",
    "                    'resize_shape': 840,\n",
    "                    'resize_longest_edge': True,\n",
    "                    'scale': True,\n",
    "                    'grayscale': True,\n",
    "                },\n",
    "                distance_threshold=None,\n",
    "                top_k=None\n",
    "            )\n",
    "            '''\n",
    "            '''\n",
    "            superglue_outputs = superglue_match_images(\n",
    "                image1=image1,\n",
    "                image2=image2,\n",
    "                model=superglue_model,\n",
    "                device=image_matching_device,\n",
    "                amp=True,\n",
    "                transforms=superglue_transforms,\n",
    "                score_threshold=None,\n",
    "                top_k=None\n",
    "            )\n",
    "            '''\n",
    "            stage2 = False\n",
    "            if stage2:\n",
    "                if superglue_outputs['keypoints0'].shape[0] > 2:\n",
    "                    image1_cropped, keypoints1_cropped, x_offset1, y_offset1 = crop(image1, superglue_outputs['keypoints0'])\n",
    "                    image2_cropped, keypoints2_cropped, x_offset2, y_offset2 = crop(image2, superglue_outputs['keypoints1'])\n",
    "\n",
    "                    image_pair_longest_edge = np.concatenate([image1_cropped.shape[:2], image2_cropped.shape[:2]]).max()\n",
    "                    if image_pair_longest_edge > 2000:\n",
    "                        superglue_transforms = {\n",
    "                            'resize': True,\n",
    "                            'resize_shape': 1920,\n",
    "                            'resize_longest_edge': True,\n",
    "                            'scale': True,\n",
    "                            'grayscale': True,\n",
    "                        }\n",
    "                    else:\n",
    "                        superglue_transforms = {\n",
    "                            'resize': False,\n",
    "                            'resize_shape': None,\n",
    "                            'resize_longest_edge': None,\n",
    "                            'scale': True,\n",
    "                            'grayscale': True,\n",
    "                        }\n",
    "\n",
    "\n",
    "                    second_stage_superglue_outputs = superglue_match_images(\n",
    "                        image1=image1_cropped,\n",
    "                        image2=image2_cropped,\n",
    "                        model=superglue_model,\n",
    "                        device=image_matching_device,\n",
    "                        amp=True,\n",
    "                        transforms=superglue_transforms,\n",
    "                        score_threshold=None,\n",
    "                        top_k=None\n",
    "                    )\n",
    "\n",
    "\n",
    "                    second_stage_superglue_outputs['keypoints0'][:, 0] += x_offset1\n",
    "                    second_stage_superglue_outputs['keypoints0'][:, 1] += y_offset1\n",
    "                    second_stage_superglue_outputs['keypoints1'][:, 0] += x_offset2\n",
    "                    second_stage_superglue_outputs['keypoints1'][:, 1] += y_offset2\n",
    "\n",
    "                    mkpts0 = np.concatenate([\n",
    "                        superglue_outputs['keypoints0'], second_stage_superglue_outputs['keypoints0']\n",
    "                    ], axis=0)\n",
    "                    mkpts1 = np.concatenate([\n",
    "                        superglue_outputs['keypoints1'],\n",
    "                        second_stage_superglue_outputs['keypoints1']\n",
    "                    ], axis=0)\n",
    "                else:\n",
    "                    mkpts0 = superglue_outputs['keypoints0']\n",
    "                    mkpts1 = superglue_outputs['keypoints1']\n",
    "            else:\n",
    "                mkpts0 = sift_outputs['keypoints0']\n",
    "                mkpts1 = sift_outputs['keypoints1']\n",
    "            \n",
    "            n_matches = len(mkpts1)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= 5:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n",
    "\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return\n",
    "\n",
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    \n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, 'simple-radial', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16292f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_scenes = {\n",
    "    #'haiper': ['bike', 'chairs', 'fountain'],\n",
    "    'heritage': ['cyprus'],\n",
    "    #'urban': ['kyiv-puppet-theater']\n",
    "}\n",
    "\n",
    "for dataset in validation_scenes.keys():\n",
    "    \n",
    "    dataset_directory = competition_dataset / 'train' / dataset\n",
    "    \n",
    "    for scene in validation_scenes[dataset]:\n",
    "        \n",
    "        df_scene = df.loc[df['scene'] == scene]\n",
    "        \n",
    "        scene_mean_memory_usage = df_scene['memory_usage'].mean()\n",
    "        if scene_mean_memory_usage <= 8:\n",
    "            lru_cache_max_size = 32\n",
    "        else:\n",
    "            lru_cache_max_size = 2\n",
    "            \n",
    "        @lru_cache(maxsize=lru_cache_max_size)\n",
    "        def read_image(image_path):\n",
    "            return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        scene_directory = dataset_directory / scene\n",
    "        scene_image_directory = scene_directory / 'images'\n",
    "        image_paths = sorted(glob(str(scene_image_directory / '*')))\n",
    "        scene_image_count = len(image_paths)\n",
    "        \n",
    "        # Select images if scene image count is above the specified threshold\n",
    "        max_scene_image_count = 200\n",
    "        if scene_image_count > max_scene_image_count:\n",
    "            # Select images with a step size\n",
    "            step_size = int(np.ceil(len(image_paths) / max_scene_image_count))\n",
    "            image_paths = image_paths[::step_size]\n",
    "            \n",
    "        feature_dir = f'featureout/{dataset}_{scene}'\n",
    "        if not os.path.isdir(feature_dir):\n",
    "            os.makedirs(feature_dir, exist_ok=True)\n",
    "            \n",
    "        scene_image_pair_indices = create_image_pairs(image_paths)\n",
    "        \n",
    "        print(\n",
    "            f'''\n",
    "            Dataset: {dataset} - Scene: {scene}\n",
    "            Image count: {scene_image_count} - Pair count: {len(scene_image_pair_indices)}\n",
    "            Average memory usage per image: {scene_mean_memory_usage:.4f} - LRU cache max size: {lru_cache_max_size}\n",
    "            '''\n",
    "        )\n",
    "        \n",
    "        match(\n",
    "            read_image,\n",
    "            image_paths,\n",
    "            scene_image_pair_indices,\n",
    "            feature_dir=feature_dir,\n",
    "            loftr_model=loftr_model,\n",
    "            superglue_model=superglue_model\n",
    "        )\n",
    "        \n",
    "        database_path = f'{feature_dir}/colmap.db'\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "\n",
    "        import_into_colmap(str(scene_directory / 'images'), feature_dir=feature_dir,database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "\n",
    "        mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "        mapper_options.min_model_size = 3\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        reconstructions = pycolmap.incremental_mapping(\n",
    "            database_path=database_path,\n",
    "            image_path=str(scene_directory / 'images'),\n",
    "            output_path=output_path,\n",
    "            options=mapper_options\n",
    "        )\n",
    "    \n",
    "        if len(reconstructions) > 0:\n",
    "\n",
    "            best_registered_image_count = 0\n",
    "            best_reconstruction_idx = None\n",
    "\n",
    "            for reconstruction_idx in reconstructions.keys():\n",
    "                if reconstructions[reconstruction_idx].num_reg_images() > best_registered_image_count:\n",
    "                    best_reconstruction_idx = reconstruction_idx\n",
    "                    best_registered_image_count = reconstructions[reconstruction_idx].num_reg_images()\n",
    "\n",
    "            best_reconstruction = reconstructions[best_reconstruction_idx]\n",
    "        else:\n",
    "            best_registered_image_count = 0\n",
    "            best_reconstruction_idx = None\n",
    "            best_reconstruction = None\n",
    "\n",
    "        print(\n",
    "            f'''\n",
    "            Dataset: {dataset} - Scene: {scene}\n",
    "            Reconstruction count: {len(reconstructions)}\n",
    "            Best reconstruction registered image count: {best_registered_image_count}/{len(image_paths)}\n",
    "            '''\n",
    "        )\n",
    "\n",
    "        if best_reconstruction is not None:\n",
    "            registered_images = {image.name: image for image in best_reconstruction.images.values()}\n",
    "        else:\n",
    "            registered_images = {}\n",
    "\n",
    "        for idx, row in df.loc[df['scene'] == scene].iterrows():\n",
    "            if row['image_id'] in registered_images:\n",
    "                rotation_matrix_prediction = deepcopy(registered_images[row['image_id']].rotmat())\n",
    "                translation_vector_prediction = deepcopy(registered_images[row['image_id']].tvec)\n",
    "                df.loc[idx, 'rotation_matrix_prediction'] = ';'.join([str(x) for x in rotation_matrix_prediction.reshape(-1)])\n",
    "                df.loc[idx, 'translation_vector_prediction'] = ';'.join([str(x) for x in translation_vector_prediction.reshape(-1)])\n",
    "            else:\n",
    "                df.loc[idx, 'rotation_matrix_prediction'] = np.nan\n",
    "                df.loc[idx, 'translation_vector_prediction'] = np.nan\n",
    "\n",
    "        # Fill unregistered images rotation matrices with the prediction mean or zeros\n",
    "        scene_rotation_matrix_predictions = df.loc[df['scene'] == scene, 'rotation_matrix_prediction'].dropna().apply(lambda x: np.array(str(x).split(';'), dtype=np.float64).reshape(1, 3, 3)).values\n",
    "        if scene_rotation_matrix_predictions.shape[0] == 0:\n",
    "            rotation_matrix_fill_value = np.zeros((3, 3))\n",
    "        else:\n",
    "            rotation_matrix_fill_value = np.mean(np.concatenate(scene_rotation_matrix_predictions, axis=0), axis=0)\n",
    "        df.loc[(df['scene'] == scene) & (df['rotation_matrix_prediction'].isnull()), 'rotation_matrix_prediction'] = ';'.join([str(x) for x in rotation_matrix_fill_value.reshape(-1)])\n",
    "\n",
    "        # Fill unregistered images translation vectors with the prediction mean or zeros\n",
    "        scene_translation_vector_predictions = df.loc[df['scene'] == scene, 'translation_vector_prediction'].dropna().apply(lambda x: np.array(str(x).split(';'), dtype=np.float64).reshape(1, 3)).values\n",
    "        if scene_translation_vector_predictions.shape[0] == 0:\n",
    "            translation_vector_fill_value = np.zeros((3, 1))\n",
    "        else:\n",
    "            translation_vector_fill_value = np.mean(np.concatenate(scene_translation_vector_predictions, axis=0), axis=0)\n",
    "        df.loc[(df['scene'] == scene) & (df['translation_vector_prediction'].isnull()), 'translation_vector_prediction'] = ';'.join([str(x) for x in translation_vector_fill_value.reshape(-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(df.dropna(subset=['rotation_matrix_prediction', 'translation_vector_prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc2601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bca3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
